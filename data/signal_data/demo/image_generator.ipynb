{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tqdm\n",
    "from scipy.signal import stft\n",
    "from Feature_extract import feature_extractors\n",
    "import torch\n",
    "import json \n",
    "class THU_006or018_basic():\n",
    "    def __init__(self, args): # 1hz, 10hz, 15hz,IF\n",
    "        self.data_loader(args.data_dir,args.target)\n",
    "        self.data_create()\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        self.save_dir = args.save_dir\n",
    "        # Load data and labels \n",
    "    def data_loader(self,data_dir,target):\n",
    "        \n",
    "        self.data = np.load(data_dir + target + '_data.npy').astype(np.float32)\n",
    "        self.labels = np.load(data_dir + target + '_label.npy').astype(np.float32)\n",
    "        \n",
    "    def data_create(self):\n",
    "       #  Define split ratios\n",
    "        train_ratio = 0.6\n",
    "        val_ratio = 0.1\n",
    "        # Calculate test_ratio to ensure ratios sum to 1\n",
    "        test_ratio = 1 - (train_ratio + val_ratio)\n",
    "\n",
    "        # Split indices for each label\n",
    "        train_indices, val_indices, test_indices = [], [], []\n",
    "\n",
    "        # TODO: Select a fixed number of samples for each label\n",
    "        selected_len = 10\n",
    "\n",
    "        for label in np.unique(self.labels):\n",
    "            label_indices = np.where(self.labels == label)[0][:selected_len]\n",
    "            # np.random.shuffle(label_indices)\n",
    "            \n",
    "            n_train = int(len(label_indices) * train_ratio)\n",
    "            n_val = int(len(label_indices) * val_ratio)\n",
    "            # Remaining indices are for testing\n",
    "            n_test = len(label_indices) - n_train - n_val\n",
    "\n",
    "            # Append indices for each set\n",
    "            train_indices.extend(label_indices[:n_train])\n",
    "            val_indices.extend(label_indices[n_train:n_train + n_val])\n",
    "            test_indices.extend(label_indices[n_train + n_val:])\n",
    "\n",
    "        # # Select indices based on the flag\n",
    "        # if self.flag == 'train':\n",
    "        #     selected_indices = train_indices\n",
    "        # elif self.flag == 'val':\n",
    "        #     selected_indices = val_indices\n",
    "        # elif self.flag == 'test':\n",
    "        #     selected_indices = test_indices\n",
    "        # else:\n",
    "        #     raise ValueError(\"Invalid flag. Please choose from 'train', 'val', or 'test'.\")\n",
    "\n",
    "        self.train_data = self.data[train_indices]\n",
    "        self.train_label = self.labels[train_indices]\n",
    "\n",
    "        self.val_data = self.data[val_indices]\n",
    "        self.val_label = self.labels[val_indices]\n",
    "\n",
    "        self.test_data = self.data[test_indices]\n",
    "        self.test_label = self.labels[test_indices]\n",
    "\n",
    "    def data_save(self,save_dir):\n",
    "        np.save(save_dir + 'train_data.npy', self.train_data)\n",
    "        np.save(save_dir + 'train_label.npy', self.train_label)\n",
    "        np.save(save_dir + 'val_data.npy', self.val_data)\n",
    "        np.save(save_dir + 'val_label.npy', self.val_label)\n",
    "        np.save(save_dir + 'test_data.npy', self.test_data)\n",
    "        np.save(save_dir + 'test_label.npy', self.test_label)\n",
    "\n",
    "    def data_plot_save(self,time_step_per_sample=1):\n",
    "\n",
    "        datas = [self.train_data, self.val_data, self.test_data]\n",
    "        labels = [self.train_label, self.val_label, self.test_label]\n",
    "        \n",
    "        \n",
    "        datasets = {\n",
    "            'train': [self.train_data, self.train_label],\n",
    "            'val': [self.val_data, self.val_label],\n",
    "            'test': [self.test_data, self.test_label]\n",
    "        }\n",
    "\n",
    "        t = np.linspace(0, time_step_per_sample,datas[0].shape[1])\n",
    "        signal_instruct = []\n",
    "\n",
    "        for key,(data,label) in datasets.items():\n",
    "            for data_index,(sample,label_sample) in tqdm.tqdm(enumerate(zip(data,label)),total=len(data),desc='Plotting data and labels'):\n",
    "                sample_one = sample[:,0]\n",
    "\n",
    "                # 时域\n",
    "                # plt.plot(t,sample_one, label='Signal')\n",
    "                # # plt.plot(label[i], label='Label')\n",
    "                # plt.xlabel('Time')\n",
    "                # plt.ylabel('Value')\n",
    "                # plt.legend()\n",
    "                # plt.savefig(self.save_dir + f'THU_006_data_label_{label_sample}' + str(data_index) + '.jpg', dpi=600)\n",
    "                # plt.close()\n",
    "                # 频域\n",
    "                # 时频图\n",
    "                # 计算短时傅里叶变换\n",
    "                frequencies, times, Zxx = stft(sample_one, fs=1.0/time_step_per_sample, nperseg=100, noverlap=50)\n",
    "\n",
    "                # 绘制时频图\n",
    "                plt.pcolormesh(times, frequencies, np.abs(Zxx), shading='gouraud')\n",
    "                plt.title('STFT Magnitude')\n",
    "                plt.ylabel('Frequency [Hz]')\n",
    "                plt.xlabel('Time [points]')\n",
    "                file_name=  f'THU_006_{key}_{label_sample}' + str(data_index) + '_stft.jpg'\n",
    "                plt.savefig(self.save_dir + file_name, dpi=600)\n",
    "                plt.close()\n",
    "                res = self.data_feature(sample_one,file_name)\n",
    "                signal_instruct.append(res)\n",
    "        with open('signal_instruct.json', 'w') as f:\n",
    "            json.dump(signal_instruct, f)\n",
    "        \n",
    "        return signal_instruct\n",
    "\n",
    "\n",
    "    def data_feature(self,sample_one,file_name):\n",
    "        res = {}\n",
    "        res['image'] = file_name\n",
    "        res['feature'] = {}\n",
    "        for key,feature_extractor in feature_extractors.items():\n",
    "            res['feature'][key] = feature_extractor(torch.tensor(sample_one)).numpy().item()\n",
    "        return res\n",
    "    \n",
    "    def basic_add_template(self):\n",
    "        pass\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting data and labels:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting data and labels: 100%|██████████| 24/24 [00:09<00:00,  2.50it/s]\n",
      "Plotting data and labels: 100%|██████████| 4/4 [00:01<00:00,  3.02it/s]\n",
      "Plotting data and labels: 100%|██████████| 12/12 [00:04<00:00,  2.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'image': 'THU_006_train_0.00_stft.jpg',\n",
       "  'feature': {'Mean': -0.0008710779948160052,\n",
       "   'Std': 0.01727261394262314,\n",
       "   'Var': 0.0002983432204928249,\n",
       "   'Entropy': 0.007543822750449181,\n",
       "   'AbsMean': 0.013559645973145962,\n",
       "   'Kurtosis': 3.5381526947021484}},\n",
       " {'image': 'THU_006_train_0.01_stft.jpg',\n",
       "  'feature': {'Mean': -0.0011105708545073867,\n",
       "   'Std': 0.017081551253795624,\n",
       "   'Var': 0.0002917794045060873,\n",
       "   'Entropy': 0.009529337286949158,\n",
       "   'AbsMean': 0.013419398106634617,\n",
       "   'Kurtosis': 3.380795955657959}},\n",
       " {'image': 'THU_006_train_0.02_stft.jpg',\n",
       "  'feature': {'Mean': -0.0010894376318901777,\n",
       "   'Std': 0.017080195248126984,\n",
       "   'Var': 0.0002917330712080002,\n",
       "   'Entropy': 0.009353507310152054,\n",
       "   'AbsMean': 0.01339738629758358,\n",
       "   'Kurtosis': 3.4682130813598633}},\n",
       " {'image': 'THU_006_train_0.03_stft.jpg',\n",
       "  'feature': {'Mean': -0.0011844004038721323,\n",
       "   'Std': 0.016963893547654152,\n",
       "   'Var': 0.00028777369880117476,\n",
       "   'Entropy': 0.010139438323676586,\n",
       "   'AbsMean': 0.013361376710236073,\n",
       "   'Kurtosis': 3.393186330795288}},\n",
       " {'image': 'THU_006_train_0.04_stft.jpg',\n",
       "  'feature': {'Mean': -0.0010342164896428585,\n",
       "   'Std': 0.01664876751601696,\n",
       "   'Var': 0.00027718141791410744,\n",
       "   'Entropy': 0.008879626169800758,\n",
       "   'AbsMean': 0.013104263693094254,\n",
       "   'Kurtosis': 3.3728604316711426}},\n",
       " {'image': 'THU_006_train_0.05_stft.jpg',\n",
       "  'feature': {'Mean': -0.0011390047147870064,\n",
       "   'Std': 0.016640253365039825,\n",
       "   'Var': 0.0002768980630207807,\n",
       "   'Entropy': 0.009750963188707829,\n",
       "   'AbsMean': 0.013024946674704552,\n",
       "   'Kurtosis': 3.4389145374298096}},\n",
       " {'image': 'THU_006_train_1.06_stft.jpg',\n",
       "  'feature': {'Mean': -0.012693178839981556,\n",
       "   'Std': 0.7655160427093506,\n",
       "   'Var': 0.5860148072242737,\n",
       "   'Entropy': 0.7309704422950745,\n",
       "   'AbsMean': 0.3255797326564789,\n",
       "   'Kurtosis': 69.57947540283203}},\n",
       " {'image': 'THU_006_train_1.07_stft.jpg',\n",
       "  'feature': {'Mean': -0.007804514840245247,\n",
       "   'Std': 0.7986893653869629,\n",
       "   'Var': 0.637904703617096,\n",
       "   'Entropy': 0.7220708131790161,\n",
       "   'AbsMean': 0.33542293310165405,\n",
       "   'Kurtosis': 76.5304946899414}},\n",
       " {'image': 'THU_006_train_1.08_stft.jpg',\n",
       "  'feature': {'Mean': -0.01364500354975462,\n",
       "   'Std': 0.767987072467804,\n",
       "   'Var': 0.5898041129112244,\n",
       "   'Entropy': 0.7251635789871216,\n",
       "   'AbsMean': 0.33506500720977783,\n",
       "   'Kurtosis': 63.33125686645508}},\n",
       " {'image': 'THU_006_train_1.09_stft.jpg',\n",
       "  'feature': {'Mean': -0.007960408926010132,\n",
       "   'Std': 0.8112511038780212,\n",
       "   'Var': 0.6581283807754517,\n",
       "   'Entropy': 0.7461792230606079,\n",
       "   'AbsMean': 0.334970623254776,\n",
       "   'Kurtosis': 76.88629150390625}},\n",
       " {'image': 'THU_006_train_1.010_stft.jpg',\n",
       "  'feature': {'Mean': -0.0004144314443692565,\n",
       "   'Std': 0.7679724097251892,\n",
       "   'Var': 0.589781641960144,\n",
       "   'Entropy': 0.594567060470581,\n",
       "   'AbsMean': 0.3224075436592102,\n",
       "   'Kurtosis': 69.64140319824219}},\n",
       " {'image': 'THU_006_train_1.011_stft.jpg',\n",
       "  'feature': {'Mean': -0.006661118473857641,\n",
       "   'Std': 0.7627261281013489,\n",
       "   'Var': 0.5817511081695557,\n",
       "   'Entropy': 0.6495811939239502,\n",
       "   'AbsMean': 0.31304189562797546,\n",
       "   'Kurtosis': 78.18301391601562}},\n",
       " {'image': 'THU_006_train_2.012_stft.jpg',\n",
       "  'feature': {'Mean': 0.00017839834617916495,\n",
       "   'Std': 0.016661908477544785,\n",
       "   'Var': 0.0002776192268356681,\n",
       "   'Entropy': -0.0012063489994034171,\n",
       "   'AbsMean': 0.012796925380825996,\n",
       "   'Kurtosis': 3.9442782402038574}},\n",
       " {'image': 'THU_006_train_2.013_stft.jpg',\n",
       "  'feature': {'Mean': 0.00010484327503945678,\n",
       "   'Std': 0.016374289989471436,\n",
       "   'Var': 0.00026811735006049275,\n",
       "   'Entropy': -0.0006040249718353152,\n",
       "   'AbsMean': 0.01256418414413929,\n",
       "   'Kurtosis': 3.9787514209747314}},\n",
       " {'image': 'THU_006_train_2.014_stft.jpg',\n",
       "  'feature': {'Mean': -0.0002662252518348396,\n",
       "   'Std': 0.01600845716893673,\n",
       "   'Var': 0.00025627072318457067,\n",
       "   'Entropy': 0.0024706420954316854,\n",
       "   'AbsMean': 0.012194212526082993,\n",
       "   'Kurtosis': 3.906385660171509}},\n",
       " {'image': 'THU_006_train_2.015_stft.jpg',\n",
       "  'feature': {'Mean': -0.0003425249014981091,\n",
       "   'Std': 0.015835458412766457,\n",
       "   'Var': 0.00025076171732507646,\n",
       "   'Entropy': 0.003099784953519702,\n",
       "   'AbsMean': 0.012119011022150517,\n",
       "   'Kurtosis': 3.9268181324005127}},\n",
       " {'image': 'THU_006_train_2.016_stft.jpg',\n",
       "  'feature': {'Mean': -0.00024317068164236844,\n",
       "   'Std': 0.015461377799510956,\n",
       "   'Var': 0.00023905419220682234,\n",
       "   'Entropy': 0.002261662157252431,\n",
       "   'AbsMean': 0.011841807514429092,\n",
       "   'Kurtosis': 3.7253315448760986}},\n",
       " {'image': 'THU_006_train_2.017_stft.jpg',\n",
       "  'feature': {'Mean': 2.14077444979921e-05,\n",
       "   'Std': 0.015254460275173187,\n",
       "   'Var': 0.00023269857047125697,\n",
       "   'Entropy': 5.457460065372288e-05,\n",
       "   'AbsMean': 0.011727632023394108,\n",
       "   'Kurtosis': 3.789705276489258}},\n",
       " {'image': 'THU_006_train_3.018_stft.jpg',\n",
       "  'feature': {'Mean': -4.391389666125178e-06,\n",
       "   'Std': 0.018702734261751175,\n",
       "   'Var': 0.00034979230258613825,\n",
       "   'Entropy': 0.00038623379077762365,\n",
       "   'AbsMean': 0.01416867133229971,\n",
       "   'Kurtosis': 4.562877178192139}},\n",
       " {'image': 'THU_006_train_3.019_stft.jpg',\n",
       "  'feature': {'Mean': -2.360348662477918e-05,\n",
       "   'Std': 0.018530545756220818,\n",
       "   'Var': 0.0003433811361901462,\n",
       "   'Entropy': 0.0005396307678893209,\n",
       "   'AbsMean': 0.014179100282490253,\n",
       "   'Kurtosis': 3.7753827571868896}},\n",
       " {'image': 'THU_006_train_3.020_stft.jpg',\n",
       "  'feature': {'Mean': -7.520182407461107e-05,\n",
       "   'Std': 0.018519947305321693,\n",
       "   'Var': 0.00034298840910196304,\n",
       "   'Entropy': 0.000968427280895412,\n",
       "   'AbsMean': 0.014008935540914536,\n",
       "   'Kurtosis': 4.67897367477417}},\n",
       " {'image': 'THU_006_train_3.021_stft.jpg',\n",
       "  'feature': {'Mean': -0.00030794297344982624,\n",
       "   'Std': 0.018361888825893402,\n",
       "   'Var': 0.00033715894096530974,\n",
       "   'Entropy': 0.0028985277749598026,\n",
       "   'AbsMean': 0.013982586562633514,\n",
       "   'Kurtosis': 3.8233001232147217}},\n",
       " {'image': 'THU_006_train_3.022_stft.jpg',\n",
       "  'feature': {'Mean': -0.00048030324978753924,\n",
       "   'Std': 0.018144793808460236,\n",
       "   'Var': 0.0003292335313744843,\n",
       "   'Entropy': 0.00432428065687418,\n",
       "   'AbsMean': 0.013674096204340458,\n",
       "   'Kurtosis': 4.842585563659668}},\n",
       " {'image': 'THU_006_train_3.023_stft.jpg',\n",
       "  'feature': {'Mean': -0.00031343221780844033,\n",
       "   'Std': 0.018041744828224182,\n",
       "   'Var': 0.00032550457399338484,\n",
       "   'Entropy': 0.0029325312934815884,\n",
       "   'AbsMean': 0.013771804049611092,\n",
       "   'Kurtosis': 3.8123981952667236}},\n",
       " {'image': 'THU_006_val_0.00_stft.jpg',\n",
       "  'feature': {'Mean': -0.0008836481138132513,\n",
       "   'Std': 0.01647588424384594,\n",
       "   'Var': 0.0002714547445066273,\n",
       "   'Entropy': 0.00762148667126894,\n",
       "   'AbsMean': 0.012928774580359459,\n",
       "   'Kurtosis': 3.4870493412017822}},\n",
       " {'image': 'THU_006_val_1.01_stft.jpg',\n",
       "  'feature': {'Mean': -0.0027237306348979473,\n",
       "   'Std': 0.7822856307029724,\n",
       "   'Var': 0.611970841884613,\n",
       "   'Entropy': 0.6441249251365662,\n",
       "   'AbsMean': 0.3278917670249939,\n",
       "   'Kurtosis': 69.69544982910156}},\n",
       " {'image': 'THU_006_val_2.02_stft.jpg',\n",
       "  'feature': {'Mean': -6.038120773155242e-06,\n",
       "   'Std': 0.01490748580545187,\n",
       "   'Var': 0.00022223315318115056,\n",
       "   'Entropy': 0.0002724029473029077,\n",
       "   'AbsMean': 0.0114367064088583,\n",
       "   'Kurtosis': 3.728175163269043}},\n",
       " {'image': 'THU_006_val_3.03_stft.jpg',\n",
       "  'feature': {'Mean': -0.00017730050603859127,\n",
       "   'Std': 0.01794927939772606,\n",
       "   'Var': 0.00032217660918831825,\n",
       "   'Entropy': 0.001796870375983417,\n",
       "   'AbsMean': 0.013557725585997105,\n",
       "   'Kurtosis': 4.579805850982666}},\n",
       " {'image': 'THU_006_test_0.00_stft.jpg',\n",
       "  'feature': {'Mean': -0.0007268772460520267,\n",
       "   'Std': 0.01641227863729,\n",
       "   'Var': 0.00026936287758871913,\n",
       "   'Entropy': 0.006315387785434723,\n",
       "   'AbsMean': 0.012866527773439884,\n",
       "   'Kurtosis': 3.5672523975372314}},\n",
       " {'image': 'THU_006_test_0.01_stft.jpg',\n",
       "  'feature': {'Mean': -0.0005117562832310796,\n",
       "   'Std': 0.016211718320846558,\n",
       "   'Var': 0.00026281984173692763,\n",
       "   'Entropy': 0.004519490525126457,\n",
       "   'AbsMean': 0.012710470706224442,\n",
       "   'Kurtosis': 3.507699966430664}},\n",
       " {'image': 'THU_006_test_0.02_stft.jpg',\n",
       "  'feature': {'Mean': -0.0006156114977784455,\n",
       "   'Std': 0.016429195180535316,\n",
       "   'Var': 0.0002699184406083077,\n",
       "   'Entropy': 0.005390448495745659,\n",
       "   'AbsMean': 0.012810043059289455,\n",
       "   'Kurtosis': 3.6925837993621826}},\n",
       " {'image': 'THU_006_test_1.03_stft.jpg',\n",
       "  'feature': {'Mean': -0.012868832796812057,\n",
       "   'Std': 0.794658362865448,\n",
       "   'Var': 0.631481945514679,\n",
       "   'Entropy': 0.7627899646759033,\n",
       "   'AbsMean': 0.333516001701355,\n",
       "   'Kurtosis': 68.74335479736328}},\n",
       " {'image': 'THU_006_test_1.04_stft.jpg',\n",
       "  'feature': {'Mean': -0.001998059917241335,\n",
       "   'Std': 0.7967087030410767,\n",
       "   'Var': 0.6347448229789734,\n",
       "   'Entropy': 0.6582071781158447,\n",
       "   'AbsMean': 0.340440034866333,\n",
       "   'Kurtosis': 67.3298568725586}},\n",
       " {'image': 'THU_006_test_1.05_stft.jpg',\n",
       "  'feature': {'Mean': -0.010675357654690742,\n",
       "   'Std': 0.7751777172088623,\n",
       "   'Var': 0.6009004712104797,\n",
       "   'Entropy': 0.7094089984893799,\n",
       "   'AbsMean': 0.32518669962882996,\n",
       "   'Kurtosis': 64.97301483154297}},\n",
       " {'image': 'THU_006_test_2.06_stft.jpg',\n",
       "  'feature': {'Mean': -0.00013338710414245725,\n",
       "   'Std': 0.014859030954539776,\n",
       "   'Var': 0.00022079081099946052,\n",
       "   'Entropy': 0.0013302339939400554,\n",
       "   'AbsMean': 0.011402673088014126,\n",
       "   'Kurtosis': 3.781137228012085}},\n",
       " {'image': 'THU_006_test_2.07_stft.jpg',\n",
       "  'feature': {'Mean': -0.0003655794425867498,\n",
       "   'Std': 0.014757206663489342,\n",
       "   'Var': 0.00021777514484710991,\n",
       "   'Entropy': 0.003258564742282033,\n",
       "   'AbsMean': 0.011255014687776566,\n",
       "   'Kurtosis': 3.8404343128204346}},\n",
       " {'image': 'THU_006_test_2.08_stft.jpg',\n",
       "  'feature': {'Mean': -0.00024975769338198006,\n",
       "   'Std': 0.01477822009474039,\n",
       "   'Var': 0.00021839576947968453,\n",
       "   'Entropy': 0.0022957967594265938,\n",
       "   'AbsMean': 0.011323629878461361,\n",
       "   'Kurtosis': 3.741295337677002}},\n",
       " {'image': 'THU_006_test_3.09_stft.jpg',\n",
       "  'feature': {'Mean': 2.8543747248477302e-05,\n",
       "   'Std': 0.018090464174747467,\n",
       "   'Var': 0.00032726486097089946,\n",
       "   'Entropy': 8.975932723842561e-05,\n",
       "   'AbsMean': 0.013853591866791248,\n",
       "   'Kurtosis': 3.87983775138855}},\n",
       " {'image': 'THU_006_test_3.010_stft.jpg',\n",
       "  'feature': {'Mean': -0.0001262511796085164,\n",
       "   'Std': 0.01788422092795372,\n",
       "   'Var': 0.0003198453923687339,\n",
       "   'Entropy': 0.0013699139235541224,\n",
       "   'AbsMean': 0.013535220175981522,\n",
       "   'Kurtosis': 4.635928630828857}},\n",
       " {'image': 'THU_006_test_3.011_stft.jpg',\n",
       "  'feature': {'Mean': -0.00020803994266316295,\n",
       "   'Std': 0.017929818481206894,\n",
       "   'Var': 0.0003214784082956612,\n",
       "   'Entropy': 0.0020518607925623655,\n",
       "   'AbsMean': 0.013667508959770203,\n",
       "   'Kurtosis': 3.9107892513275146}}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace()\n",
    "args.data_dir = '/home/user/data/a_bearing/a_006_THU_pro/'\n",
    "args.target = '10hz'\n",
    "args.save_dir = '/home/user/LQ/B_Signal/LLava_signal/data/signal_data/demo/save_dir_stft/'\n",
    "\n",
    "dataset = THU_006or018_basic(args)\n",
    "dataset.data_plot_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# def describe_features(features):\n",
    "#     \"\"\"\n",
    "#     Generate a description for each feature in a dictionary.\n",
    "#     \"\"\"\n",
    "#     descriptions = [\n",
    "#         f\"The Mean of the signal is {features['Mean']:.4f}.\",\n",
    "#         f\"The Standard Deviation (Std) of the signal is {features['Std']:.4f}.\",\n",
    "#         f\"The Variance (Var) of the signal is {features['Var']:.4f}.\",\n",
    "#         f\"The Entropy of the signal is {features['Entropy']:.4f}.\",\n",
    "#         f\"The Absolute Mean (AbsMean) of the signal is {features['AbsMean']:.4f}.\",\n",
    "#         f\"The Kurtosis of the signal is {features['Kurtosis']:.4f}.\"\n",
    "#     ]\n",
    "#     return descriptions\n",
    "\n",
    "def describe_features(features):\n",
    "    \"\"\"\n",
    "    Generate a description for each feature in a dictionary by iterating over key-value pairs.\n",
    "    \"\"\"\n",
    "    descriptions = []\n",
    "    questions_title = [\"What is the\", \"Describe the\", \"What does the\", \"Explain the\", \"What can you tell me about the\"]\n",
    "    question = []\n",
    "    for key, value in features.items():\n",
    "        # Format the key to be more readable, replacing underscores with spaces and capitalizing each word\n",
    "        readable_key = ' '.join(word.capitalize() for word in key.split('_'))\n",
    "        descriptions.append(f\"The {readable_key} of the signal is {value:.4f}.\")\n",
    "        question.append(f\"{questions_title[np.random.randint(0, len(questions_title))]} {readable_key} of the signal?\")\n",
    "\n",
    "    return question,descriptions\n",
    "\n",
    "def extract_health_status(image_name):\n",
    "    \"\"\"\n",
    "    Extract health status from the image file name based on the first decimal digit.\n",
    "    \"\"\"\n",
    "    # Splitting the filename to extract the relevant part which contains the health status indicator\n",
    "    health_indicator = image_name.split('_')[3]\n",
    "    # Extracting the first decimal digit\n",
    "    first_decimal_digit = int(health_indicator.split('.')[1][0])\n",
    "    \n",
    "    # Determining the health status based on the first decimal digit\n",
    "    health_status = f\"healthy_{first_decimal_digit}\" if first_decimal_digit == 0 else f\"unhealthy{first_decimal_digit}\"\n",
    "    return health_status\n",
    "\n",
    "def transform_data(input_data,save_dir = 'save_dir_stft/'):\n",
    "    \"\"\"\n",
    "    Transform the input data into the specified format with conversations.\n",
    "    \"\"\"\n",
    "    transformed_data = []\n",
    "\n",
    "    for id,item in enumerate(input_data):\n",
    "        health_status = extract_health_status(item['image'])\n",
    "        # health_status = \"healthy\" if float(item['image'].split('_')[2]) == 0.00 else \"unhealthy\"\n",
    "        question,feature_descriptions = describe_features(item['feature'])\n",
    "        \n",
    "        conversation = []\n",
    "\n",
    "\n",
    "        conversation.append({\"from\": \"human\", \"value\": \"<image>\\nDescribe a feature of this signal.\"})\n",
    "        conversation.append({\"from\": \"gpt\", \"value\": feature_descriptions[0]})\n",
    "\n",
    "        for desc in feature_descriptions[1:]:\n",
    "            conversation.append({\"from\": \"human\", \"value\": question})  # what is the \n",
    "            conversation.append({\"from\": \"gpt\", \"value\": desc})\n",
    "\n",
    "\n",
    "        # final\n",
    "        conversation.append({\"from\": \"human\", \"value\": f\"Describe the health status of this signal.\"})\n",
    "        conversation.append({\"from\": \"gpt\", \"value\": f\"This signal is categorized as {health_status} based on its characteristics.\"})\n",
    "\n",
    "        transformed_item = {\n",
    "            \"id\": f\"{id}\",\n",
    "            \"image\": f\"{save_dir}{item['image']}\",\n",
    "            \"conversations\": conversation\n",
    "        }\n",
    "        transformed_data.append(transformed_item)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "# 打开并读取json文件\n",
    "with open('signal_instruct.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# Transforming the data\n",
    "transformed_data = transform_data(data)\n",
    "\n",
    "# Printing out the transformed data\n",
    "with open('signal_instruct_data.json', 'w') as f:\n",
    "    json.dump(transformed_data, f)\n",
    "\n",
    "\n",
    "# print(json.dumps(transformed_data, indent=4))\n",
    "\n",
    "# # Example input data\n",
    "# input_data = [\n",
    "#     {\"image\": \"THU_006_train_0.00_stft.jpg\", \"feature\": {\"Mean\": -0.0008710779948160052, \"Std\": 0.01727261394262314, \"Var\": 0.0002983432204928249, \"Entropy\": 0.007543822750449181, \"AbsMean\": 0.013559645973145962, \"Kurtosis\": 3.5381526947021484}}\n",
    "# ]\n",
    "\n",
    "# # Transforming the data\n",
    "# transformed_data = transform_data(input_data)\n",
    "\n",
    "# # Printing out the transformed data\n",
    "# print(json.dumps(transformed_data, indent=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtuner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
